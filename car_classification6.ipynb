{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/connoro28/LocationApp/blob/main/car_classification6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44bf24f6-8962-4658-8845-5e00e0f67f20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44bf24f6-8962-4658-8845-5e00e0f67f20",
        "outputId": "dfe72180-151f-4c9f-9738-57879cd5a4c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# PyTorch core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Torchvision for computer vision tasks\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "\n",
        "# For data handling and splitting\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For image loading and handling\n",
        "from PIL import Image\n",
        "\n",
        "# Other useful libraries\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the zip file in your Google Drive\n",
        "zip_path = '/content/drive/MyDrive/car_dataset_colab/all_images_flat.zip'\n",
        "\n",
        "# Path where you want to extract the images in the Colab environment\n",
        "extract_path = '/content/images/'\n",
        "\n",
        "print(\"Starting to unzip images...\")\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "print(\"Unzipping complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6p-pmumF8FXG",
        "outputId": "1f0da09c-7f0e-4084-88ad-7d5e69d8fe16"
      },
      "id": "6p-pmumF8FXG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to unzip images...\n",
            "Unzipping complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eef20cc-8ef7-445f-8549-65fda188b566",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eef20cc-8ef7-445f-8549-65fda188b566",
        "outputId": "8e1ece17-0345-4b4b-ea44-a1ea53df1bcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame successfully restructured.\n",
            "\n",
            "Verifying the final, Colab paths...\n",
            "\n",
            "SUCCESS: The first file was found at: /content/images/all_images_flat/AC__428_Convertible_19661971_ac-428-convertible-1966-7054_1.jpg\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import ast\n",
        "\n",
        "# --- 1. Define ABSOLUTE paths for Colab ---\n",
        "# Path to your project folder in Google Drive\n",
        "drive_project_path = r'/content/drive/MyDrive/car_dataset_colab'\n",
        "\n",
        "# Path to the Excel file in Drive\n",
        "excel_path = os.path.join(drive_project_path, 'car_data_full.xlsx')\n",
        "\n",
        "# Path to the UNZIPPED images on Colab's fast local storage\n",
        "base_image_dir = os.path.join(extract_path, 'all_images_flat')\n",
        "\n",
        "# --- 2. Load and Restructure the DataFrame ---\n",
        "# (The rest of this cell remains the same as your working version)\n",
        "df = pd.read_excel(excel_path)\n",
        "df['image_file_names'] = df['image_file_names'].apply(ast.literal_eval)\n",
        "df_exploded = df.explode('image_file_names').reset_index(drop=True)\n",
        "print(\"DataFrame successfully restructured.\")\n",
        "\n",
        "# --- 3. Create the correct Full Image Path ---\n",
        "def get_prefix_from_path(path_str):\n",
        "    parts = str(path_str).replace('\\\\', '/').split('/')\n",
        "    if len(parts) >= 2:\n",
        "        return f\"{parts[-2]}_{parts[-1]}\"\n",
        "    return \"\"\n",
        "\n",
        "df_exploded['prefix'] = df_exploded['dir_path'].apply(get_prefix_from_path)\n",
        "df_exploded['image_file_names'] = df_exploded['image_file_names'].astype(str)\n",
        "df_exploded['new_filename'] = df_exploded['prefix'] + '_' + df_exploded['image_file_names']\n",
        "df_exploded['Full Image Path'] = df_exploded['new_filename'].apply(\n",
        "    lambda filename: os.path.join(base_image_dir, filename)\n",
        ")\n",
        "\n",
        "# --- 4. Verification and Sanity Check ---\n",
        "print(\"\\nVerifying the final, Colab paths...\")\n",
        "first_path = df_exploded['Full Image Path'].iloc[0]\n",
        "if os.path.exists(first_path):\n",
        "    print(f\"\\nSUCCESS: The first file was found at: {first_path}\")\n",
        "else:\n",
        "    print(f\"\\nFAILURE: The first file was NOT found at: {first_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "193d7bb8-8f11-4b4a-b3c8-47ab34e6ba90",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "193d7bb8-8f11-4b4a-b3c8-47ab34e6ba90",
        "outputId": "8aadd891-ae32-4750-d830-5db56aacb173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of rows: 194727\n",
            "Number of rows after initial filtering: 194727\n",
            "------------------------------\n",
            "Training samples: 136308\n",
            "Validation samples: 29209\n",
            "Testing samples: 29210\n"
          ]
        }
      ],
      "source": [
        "# --- First, filter the main DataFrame ---\n",
        "brand_counts = df_exploded['brand'].value_counts()\n",
        "brands_to_keep = brand_counts[brand_counts > 1].index\n",
        "df_filtered = df_exploded[df_exploded['brand'].isin(brands_to_keep)].copy()\n",
        "\n",
        "print(f\"Original number of rows: {len(df_exploded)}\")\n",
        "print(f\"Number of rows after initial filtering: {len(df_filtered)}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- Perform the first split ---\n",
        "\n",
        "features = df_filtered['Full Image Path']\n",
        "labels = df_filtered['brand']\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    features, labels, test_size=0.3, random_state=42, stratify=labels)\n",
        "\n",
        "\n",
        "# --- FIX: Filter the temporary set AGAIN before the second split ---\n",
        "temp_brand_counts = y_temp.value_counts()\n",
        "brands_to_keep_in_temp = temp_brand_counts[temp_brand_counts > 1].index\n",
        "\n",
        "# Keep only the rows in X_temp and y_temp that correspond to the brands we want to keep\n",
        "X_temp_filtered = X_temp[y_temp.isin(brands_to_keep_in_temp)]\n",
        "y_temp_filtered = y_temp[y_temp.isin(brands_to_keep_in_temp)]\n",
        "\n",
        "\n",
        "# --- Now, perform the second split on the filtered temporary data ---\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp_filtered, y_temp_filtered, test_size=0.5, random_state=42, stratify=y_temp_filtered)\n",
        "\n",
        "\n",
        "# --- Final verification ---\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Validation samples: {len(X_val)}\")\n",
        "print(f\"Testing samples: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d08c6e8-3d50-4980-af80-dd0c170e8081",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d08c6e8-3d50-4980-af80-dd0c170e8081",
        "outputId": "06e9cdf2-bb5a-49af-c2ff-fd3527c0d7db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image transformation pipelines defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# Define the image transformations for the training set\n",
        "# Includes data augmentation\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),             # Resize the smaller edge to 256\n",
        "    transforms.RandomResizedCrop(224),  # Crop a random 224x224 part\n",
        "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
        "    transforms.ToTensor(),              # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(               # Normalize with ImageNet's mean and std\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "# Define the image transformations for the validation and test sets\n",
        "# No data augmentation\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),             # Resize the smaller edge to 256\n",
        "    transforms.CenterCrop(224),         # Crop the center 224x224 part\n",
        "    transforms.ToTensor(),              # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(               # Normalize with ImageNet's mean and std\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "print(\"Image transformation pipelines defined successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b287db0-3512-41f9-9143-dfef3cbcaed1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b287db0-3512-41f9-9143-dfef3cbcaed1",
        "outputId": "66ec80da-280b-41bb-ef64-bdded27f5072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CarDataset class defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Create a mapping from brand names to integer labels ---\n",
        "# Get a list of unique brand names from the original filtered labels\n",
        "all_labels = df_filtered['brand'].unique()\n",
        "# Create the mapping\n",
        "class_to_idx = {label: i for i, label in enumerate(all_labels)}\n",
        "# Also create the reverse mapping to get the name back from a number\n",
        "idx_to_class = {i: label for label, i in class_to_idx.items()}\n",
        "\n",
        "# --- 2. Define the Custom Dataset Class ---\n",
        "class CarDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, class_to_idx, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_paths (pandas.Series): A pandas Series of image file paths.\n",
        "            labels (pandas.Series): A pandas Series of corresponding labels.\n",
        "            class_to_idx (dict): A dictionary mapping class names to indices.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        # We need to use .values to get the underlying numpy arrays\n",
        "        self.image_paths = image_paths.values\n",
        "        self.labels = labels.values\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        # This returns the total number of samples in the dataset\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # This method gets one sample from the dataset\n",
        "        # 1. Get the image path\n",
        "        img_path = self.image_paths[idx]\n",
        "\n",
        "        # 2. Open the image using Pillow\n",
        "        # We use a try-except block to handle potentially corrupt images\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            #print(f\"Warning: Could not load image {img_path}. Skipping. Error: {e}\")\n",
        "            # Return the first image and label as a fallback\n",
        "            img_path = self.image_paths[0]\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # 3. Get the corresponding text label and convert it to its integer index\n",
        "        label_name = self.labels[idx]\n",
        "        label_idx = self.class_to_idx[label_name]\n",
        "\n",
        "        # 4. Apply transformations, if any\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label_idx\n",
        "\n",
        "print(\"CarDataset class defined successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c701fb69-1ad6-4b90-8e64-994c9cea4bed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c701fb69-1ad6-4b90-8e64-994c9cea4bed",
        "outputId": "40ffb487-2e16-4128-d016-c1b2adae68d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized DataLoaders created successfully!\n",
            "Number of batches in train_loader: 1065\n",
            "Number of batches in val_loader: 229\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Create the Dataset objects ---\n",
        "\n",
        "# Training dataset with data augmentation\n",
        "train_dataset = CarDataset(\n",
        "    image_paths=X_train,\n",
        "    labels=y_train,\n",
        "    class_to_idx=class_to_idx,\n",
        "    transform=train_transforms\n",
        ")\n",
        "\n",
        "# Validation dataset with standard transformations\n",
        "val_dataset = CarDataset(\n",
        "    image_paths=X_val,\n",
        "    labels=y_val,\n",
        "    class_to_idx=class_to_idx,\n",
        "    transform=val_test_transforms\n",
        ")\n",
        "\n",
        "# Test dataset with standard transformations\n",
        "test_dataset = CarDataset(\n",
        "    image_paths=X_test,\n",
        "    labels=y_test,\n",
        "    class_to_idx=class_to_idx,\n",
        "    transform=val_test_transforms\n",
        ")\n",
        "\n",
        "\n",
        "# --- 2. Create the DataLoader objects (UPDATED FOR SPEED) ---\n",
        "\n",
        "# Define the batch size\n",
        "batch_size = 128\n",
        "\n",
        "# Training DataLoader (shuffled to ensure random batches each epoch)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,  # <-- THE FIX: Use 2 parallel workers to load data\n",
        "    pin_memory=True # <-- Helps speed up data transfer to the GPU\n",
        ")\n",
        "\n",
        "# Validation DataLoader (no need to shuffle)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,  # <-- THE FIX\n",
        "    pin_memory=True # <-- THE FIX\n",
        ")\n",
        "\n",
        "# Test DataLoader (no need to shuffle)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,  # <-- THE FIX\n",
        "    pin_memory=True # <-- THE FIX\n",
        ")\n",
        "\n",
        "print(\"Optimized DataLoaders created successfully!\")\n",
        "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
        "print(f\"Number of batches in val_loader: {len(val_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9a8c324-09fd-4ec0-8a8f-9aa2289e7ddc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9a8c324-09fd-4ec0-8a8f-9aa2289e7ddc",
        "outputId": "a7963f3b-46c6-4019-fa16-8f86856eeb3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Model architecture updated successfully!\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Set up the device (use GPU if available, otherwise CPU) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- 2. Load the pre-trained model ---\n",
        "# We use ResNet50, with weights pre-trained on the ImageNet dataset\n",
        "model = models.resnet50(weights='IMAGENET1K_V1')\n",
        "\n",
        "# --- 3. Freeze the parameters of the base model ---\n",
        "# We don't want to change the pre-learned features during initial training\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# --- 4. Replace the final classifier layer ---\n",
        "# Get the number of input features for the classifier\n",
        "num_ftrs = model.fc.in_features\n",
        "\n",
        "# Get the number of classes (car brands) from our dataset\n",
        "num_classes = len(class_to_idx)\n",
        "\n",
        "# Create a new, unfrozen fully-connected layer to replace the old one\n",
        "# This new layer's weights WILL be updated during training\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "# --- 5. Move the model to the selected device ---\n",
        "model = model.to(device)\n",
        "\n",
        "# (Optional) Print the model architecture to see the new final layer\n",
        "print(\"\\nModel architecture updated successfully!\")\n",
        "#print(model) # Uncomment this line if you want to see the full architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c7410f4-f2c5-4003-88a2-21c2097fb915",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c7410f4-f2c5-4003-88a2-21c2097fb915",
        "outputId": "d41b5952-1497-4f18-db4a-5da44ec2f5b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss function and optimizer defined successfully.\n"
          ]
        }
      ],
      "source": [
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "# We only want to train the parameters of the new classifier layer\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Loss function and optimizer defined successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46f29616-a9f4-4cc1-a9b3-6b5517ce6555",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "46f29616-a9f4-4cc1-a9b3-6b5517ce6555",
        "outputId": "38b6434e-75cb-4420-aa24-777bb95a9513"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training with Automatic Mixed Precision...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-23-4196050525.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 | Train Loss: 3.4887 | Train Acc: 0.1433 | Val Loss: 3.3658 | Val Acc: 0.1821\n",
            "Epoch 2/10 | Train Loss: 3.2975 | Train Acc: 0.1773 | Val Loss: 3.3290 | Val Acc: 0.1903\n",
            "Epoch 3/10 | Train Loss: 3.2402 | Train Acc: 0.1880 | Val Loss: 3.2805 | Val Acc: 0.1981\n",
            "Epoch 4/10 | Train Loss: 3.1949 | Train Acc: 0.1975 | Val Loss: 3.3042 | Val Acc: 0.2050\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-23-4196050525.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mrunning_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# A function to calculate accuracy\n",
        "def calculate_accuracy(y_pred, y_true):\n",
        "    _, predicted = torch.max(y_pred.data, 1)\n",
        "    total = y_true.size(0)\n",
        "    correct = (predicted == y_true).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "# --- AMP: Initialize the Gradient Scaler ---\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Define the number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "print(\"Starting training with Automatic Mixed Precision...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # --- AMP: Wrap the forward pass in autocast ---\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        # --- AMP: Scale the loss and call backward() ---\n",
        "        # Instead of loss.backward()\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # --- AMP: Unscales gradients and calls optimizer.step() ---\n",
        "        # Instead of optimizer.step()\n",
        "        scaler.step(optimizer)\n",
        "\n",
        "        # --- AMP: Update the scale for next iteration ---\n",
        "        scaler.update()\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Gather statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_acc += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = running_acc / len(train_dataset)\n",
        "\n",
        "    # --- Validation Phase (no changes needed here) ---\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            val_acc += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
        "\n",
        "    epoch_val_loss = val_loss / len(val_dataset)\n",
        "    epoch_val_acc = val_acc / len(val_dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "          f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f} | \"\n",
        "          f\"Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\nTraining finished in {(end_time - start_time)/60:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b6d3eb1-6eb7-4279-8e28-d93c7cf5a7b7",
      "metadata": {
        "id": "3b6d3eb1-6eb7-4279-8e28-d93c7cf5a7b7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}